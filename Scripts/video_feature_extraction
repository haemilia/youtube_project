################# Needs to be fixed
#%%
from pathlib import Path
import useful as use
import pandas as pd
import numpy as np
import logging
import pickle
import sys
from datetime import datetime
import signal
import time  # For a small delay

from demo import download_video_with_id
import feature_extraction as fe

priv = use.get_priv()
global DATA_PATH
DATA_PATH = Path(priv["DATA_PATH"])
hf_token = priv["HF_API_KEY"]
features_dir = Path("../Datasets")

# load previous attempt
temp_dir = Path("../Temp")

xclip_filename = temp_dir / "unpopular_xclip_features.pkl"
l1_filename = temp_dir / "unpopular_l1_norms.pkl"

if (temp_dir / "need_to_collect.pkl").exists():
    with open(temp_dir / "need_to_collect.pkl", "rb") as fr:
        need_to_collect = pickle.load(fr)
    with open(xclip_filename, "rb") as fr:
        xclip_features_dict = pickle.load(fr)
    with open(l1_filename, "rb") as fr:
        l1_features_dict = pickle.load(fr)
else:
    unpopular_ids = pd.read_pickle(temp_dir / "unpopular_ids.pkl")
    with open(temp_dir / "unpopular_l1_norms.pkl", "rb") as fr:
        previous_attempt_l1 = pickle.load(fr)
    with open(temp_dir / "unpopular_video_features.pkl", "rb") as fr:
        previous_attempt_xclip = pickle.load(fr)

    xclip_features_dict = {}
    l1_features_dict = {}
    need_to_collect = []

    for vid_id in unpopular_ids:
        if (vid_id not in previous_attempt_l1.keys()) or (vid_id not in previous_attempt_xclip.keys()):
            need_to_collect.append(vid_id)
        else:
            xclip_features_dict[vid_id] = previous_attempt_xclip[vid_id]
            l1_features_dict[vid_id] = previous_attempt_l1[vid_id]
    with open(temp_dir / "need_to_collect.pkl", "wb") as fw:
        pickle.dump(need_to_collect, fw)

#%%
# Prepare model and preprocessor
device = fe.set_device()
vid_processor = fe.load_video_processor(hf_token)
vid_model = fe.load_video_model(device, hf_token)

progress_save_path = Path("../LOG/progress_num.txt")

if (progress_save_path).exists():
    with open(progress_save_path, "r") as fr:
        start_from = int(fr.read())
else:
    start_from = 0

error_message = "Completed without catching error"

# Define flag to indicate if graceful stop is required
graceful_stop = False

def check_for_stop_command():
    global graceful_stop
    import msvcrt
    if msvcrt.kbhit():
        key = msvcrt.getch().decode('utf-8').lower()
        if key == 'q':
            print("\n'q' pressed. Graceful stop requested. Finishing current video processing.")
            graceful_stop = True
        # msvcrt.putch(key.encode()) # Optionally echo the key press
#%%
# Define the full path to the log file on the shared drive
# changed to local path for now
shared_drive_path = Path("../LOG")
log_file_path = shared_drive_path / "progress.log"

# Ensure the directory exists
shared_drive_path.mkdir(parents=True, exist_ok=True)

# Configure logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

# File handler
file_handler = logging.FileHandler(log_file_path, mode='a')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# Console handler
stream_handler = logging.StreamHandler(sys.stdout)
stream_handler.setFormatter(formatter)
logger.addHandler(stream_handler)
#%%
ids_to_query = need_to_collect[start_from:]
total = len(ids_to_query)

try:
    import msvcrt  # Import the msvcrt module for non-blocking input on Windows

    for i, video_id in enumerate(ids_to_query):
        check_for_stop_command()
        if graceful_stop:
            logger.info("Graceful stop initiated. Exiting after this video.")
            break
        log_message = f"{i+1} / {total}"
        logger.info(log_message)
        # download video to temp
        vid_path = download_video_with_id(video_id, temp_dir, cookie_path="../cookies.txt")

        # extract features
        log_message = "Extracting feature vectors..."
        logger.info(log_message)
        try:
            xclip_features, l1_features = fe.feature_from_video_with_l1(vid_path,
                                                                        processor=vid_processor,
                                                                        model=vid_model,
                                                                        device=device,
                                                                        sample_percentage=0.05)
        except Exception as e:
            log_message = "Error while extracting feature vectors: " + str(e)
            logger.error(log_message)
            continue

        # store features
        log_message = "Storing feature vectors in dictionaries..."
        logger.info(log_message)
        xclip_features_dict[video_id] = xclip_features
        l1_features_dict[video_id] = l1_features

        with open(progress_save_path, "w") as fw:
            fw.write(str(start_from +i + 1)) # Increment start_from here

        # delete video from temp
        if vid_path is None:
            vid_path = temp_dir / f"{video_id}.mp4"
        vid_path.unlink(missing_ok=True)
        log_message = f"Deleted {video_id}.mp4"
        logger.info(log_message)
        time.sleep(0.1) # Small delay to prevent busy-waiting

except Exception as e:
    error_message = str(e)

log_message = "Storing all feature vectors..."
logger.info(log_message)
with open(xclip_filename, "wb") as fw:
    pickle.dump(xclip_features_dict, fw)
with open(l1_filename,'wb') as fw:
    pickle.dump(l1_features_dict, fw)

def write_termination_file_pathlib(reason, shared_drive_path_str):
    shared_drive_path = Path(shared_drive_path_str)
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"URGENT_termination_status_{timestamp}.txt"
    full_path = shared_drive_path / filename

    try:
        full_path.write_text(f"Loop terminated at: {timestamp}\nReason: {reason}\n")
        logger.info(f"Termination status written to: {full_path}")
    except Exception as e:
        logger.error(f"Error writing to shared drive: {e}")

write_termination_file_pathlib(error_message, Path("../LOG"))

#######################################################################################
#%%
log_message = "Updating dataset..."
logger.info(log_message)

import pandas as pd
import numpy as np
from pathlib import Path
import pickle

temp_dir = Path("../Temp")
dataset_dir = Path("../Datasets")

with open(temp_dir / "popular_l1_norms.pkl", "rb") as fr:
    popular_l1 = pickle.load(fr)

with open(temp_dir / "unpopular_l1_norms.pkl", "rb") as fr:
    unpopular_l1 = pickle.load(fr)

with open(temp_dir / "popular_xclip_features.pkl", "rb") as fr:
    popular_xclip = pickle.load(fr)

with open(temp_dir / "unpopular_xclip_features.pkl", "rb") as fr:
    unpopular_xclip = pickle.load(fr)

# Even though we don't have enough unpopular vids, I'm putting the dataset together
# If possible, run again when more unpopular vids have been collected.


# Process l1_norms
def extract_fixed_length_features(l1_norms, num_segments=5, percentile_values=[10, 30, 50, 70, 90], num_bins=10):
    """
    Extracts a fixed-length feature vector from a sequence of L1 norms.

    Args:
        l1_norms (np.ndarray): A 1D NumPy array representing the sequence of L1 norms.
        num_segments (int): The number of segments to divide the L1-norm sequence into.
        percentile_values (list): A list of percentile values to calculate (e.g., [25, 50, 75]).
        num_bins (int): The number of bins for the histogram.

    Returns:
        np.ndarray: A 1D NumPy array containing the fixed-length feature vector.
    """
    features = []
    n = len(l1_norms)

    # 1. Segment-based statistics
    if n > 0 and num_segments > 0:
        segment_size = n // num_segments
        remainder = n % num_segments
        start = 0
        for i in range(num_segments):
            end = start + segment_size + (1 if i < remainder else 0)
            if start < end:
                segment = l1_norms[start:end]
                features.extend([np.mean(segment), np.std(segment)])
            start = end
    elif n > 0:
        # Handle case where num_segments is 0 or sequence is too short
        features.extend([np.mean(l1_norms), np.std(l1_norms)])
    else:
        # Handle empty sequence
        features.extend([0.0] * 2 * num_segments) # Add placeholders

    # 2. Percentiles
    percentile_features = np.percentile(l1_norms, percentile_values)
    features.extend(percentile_features)

    # 3. Histogram
    hist, _ = np.histogram(l1_norms, bins=num_bins)
    features.extend(hist)

    return np.array(features)

popular_l1_features = {}
for video_id, l1_norms in popular_l1.items():
    l1_features = extract_fixed_length_features(np.array(l1_norms))
    popular_l1_features[video_id] = l1_features

video_ids = popular_l1_features.keys()
l1_features = [popular_l1_features[vid_id] for vid_id in video_ids]
num_elements = l1_features[0].shape[0]
column_names = [f"l1_{i}"for i in range(num_elements)]
popular_l1_df = pd.DataFrame(l1_features, index=video_ids,columns=column_names)
popular_l1_df = popular_l1_df.reset_index()
popular_l1_df = popular_l1_df.rename(columns={"index": "video_id"})
popular_l1_df["target"] = 1


unpopular_l1_features = {}
for video_id, l1_norms in unpopular_l1.items():
    if l1_norms is None:
        continue
    l1_features = extract_fixed_length_features(np.array(l1_norms))
    unpopular_l1_features[video_id] = l1_features

video_ids = unpopular_l1_features.keys()
l1_features = [unpopular_l1_features[vid_id] for vid_id in video_ids]
num_elements = l1_features[0].shape[0]
column_names = [f"l1_{i}"for i in range(num_elements)]
unpopular_l1_df = pd.DataFrame(l1_features, index=video_ids,columns=column_names)
unpopular_l1_df = unpopular_l1_df.reset_index()
unpopular_l1_df = unpopular_l1_df.rename(columns={"index": "video_id"})
unpopular_l1_df["target"] = 0

video_l1_features = pd.concat([popular_l1_df, unpopular_l1_df])
video_l1_features.to_pickle(dataset_dir / "video_l1_features.pkl")

# now xclip features
popular_xclip_features = {video_id: xclip_features for video_id, xclip_features in popular_xclip.items() if xclip_features is not None}
video_ids = popular_xclip_features.keys()
xclip_features = [popular_xclip_features[vid_id] for vid_id in video_ids]
num_elements = xclip_features[0].shape[0]
column_names = [f"xclip_{i}" for i in range(num_elements)]
df = pd.DataFrame(xclip_features, 
                  index=video_ids,
                  columns=column_names)
df = df.reset_index()
df = df.rename(columns={"index":"video_id"})
df["target"] = 1
popular_xclip_df = df


unpopular_xclip_features = {video_id: xclip_features for video_id, xclip_features in unpopular_xclip.items() if xclip_features is not None}
video_ids = unpopular_xclip_features.keys()
xclip_features = [unpopular_xclip_features[vid_id] for vid_id in video_ids]
num_elements = xclip_features[0].shape[0]
column_names = [f"xclip_{i}" for i in range(num_elements)]
df = pd.DataFrame(xclip_features, 
                  index=video_ids,
                  columns=column_names)
df = df.reset_index()
df = df.rename(columns={"index":"video_id"})
df["target"] = 1
unpopular_xclip_df = df

video_xclip_features = pd.concat([popular_xclip_df, unpopular_xclip_df])
video_xclip_features.to_pickle(dataset_dir / "video_xclip_features.pkl")
pd.merge(left=video_xclip_features, right=video_l1_features, how="inner", on=["video_id", "target"])["target"].unique()
